config.py
    boot some or all quabos
    (based on obs_config.json)

start.py
    start data recorders
    tell quabos to start sending data

qc.py
    control an individual booted quabo, given its IP address

hashpipe config
    needs to know what quabo IP addrs to expect data from
    if get packet from elsewhere, write error msg to log file
    if get malformed packet, write msg to log file
---------
1/24/2022
config.py
    do everything from power-on to start record
    re-do if any change to config (except data_config).
    Maybe keep track of where we are in the process?

    mobo serial#: written on board

    module ID: last 10 bits of base IP addr >> 2

    Need to add all calibration stuff
    (Wei can help me)
    
start/stop.py

status.py
    to be written

web tools

-------------
notion of "mobo ID": the base IP addr
hashpipe cmdline:
    list of mobo IDs
    name of log file
bad source (from board loc) => log
malformed packet -> log
---------
1/25/22
In experimenting with Ryan, we found
- if you run hashpipe from a python script,
    it exits when the script exist (we don't know why)
    If you run a shell script from a python script,
    and the shell script runs hashpipe,
    then both shell script and hashpipe continue to run
    after python script exist.
    That's good.
    But killing the shell script doesn't kill hashpipe.
    However, killing hashpipe causes the shell script to exit

So we came up with the following scheme

starting hashpipe
    start.py (on head node)
        for each DAQ node
            copy to node:
                run_hashpipe.sh
                module.config
                config files
                start_daq.py
            ssh to run start_daq.py

    start_daq.py (on DAQ node)
        args: run name, file size, module IDs
        create run_hashpipe.sh
            hashpipe command line, including
                SAVELOC: path to run dir
                MAXFILESIZE: from data config
                redirects stderr/stdout
        create module.config
            list of module IDs
        run run_hashpipe.sh (in new session, detach)
        use pgrep to find PID of the hashpipe process
        write that to a daq_pid

    run_hashpipe.sh
        runs hashpipe, passing cmdline args

    hashpipe
        reads module.config
------------
1/26/22:
did:
    start hashpipe remote with start/stop scripts
        python on head node / python on DAQ/ shell script / hashpip
    tell it what quabos to expect data from:
        notion of "module ID": bits 2..10 of base IP addr
    hashpipe doesn't:
        create run dir
        record HK data
todo, short-term:
    some hashpipe changes
    finish start.py
    actually record data
    redirect hashpipe stdout/stderr to log files, collect these

longer term
    calibration
    hashpipe:
        orient 32x32 images
        integrate pulse-finding
------------
1/27/22

Definition of "run in progress":
A run is in progress if there's a file "run_name"
    in the data dir on the head node.
start.py creates this before it starts anything.
    it complains if it already exists
stop.py:
    if there's a run in progess, terminate it normally,
        e.g. collect its data
        and deletes "run_name".

A run involves "recording activities" on distributed nodes,
There are two kinds of "recording activities":
1) recording HK data (store_redis_data.py on the head node)
2) a hashpipe process

Normally, start.py starts these activities,
and stop.py stops them.
So there always at most one instance of the activity -
e.g. at most one hashpipe process on a host.

But it's possible a recording activity could be active
even after stop.py has been called,
perhaps because stop.py failed for some reason partway through

It would be bad if more than one instance of a recording activity
was active at the same time -
for example, you wouldn't want two hashpipe processes running at once.
So we want to prevent this from happening.

Here's how we do it:
start.py:
    fail if some recording activity was already active
    (possibly leave some recording activities active)
    in particular:
        local: fail if HK record running
        start_daq.py: fail if an instance of hashpipe already running
    tell user:
        you or another 

stop.py: (in addition to the above)
    if there is no run in progress:
        kill HK recording
        kill all hashpipes on DAQ nodes
            stop_daq.py:
                if no current run,
                kill all hashpipe processes

status.py:
    if no run in progress:
        show status of recording activities
            (use status_daq.py for remote nodes)

So for the end user:

- if your start.py fails because something already active,
    just do stop.py and try again.

- if you want to make sure your start.py won't fail for this reason,
    do "status.py".  If it shows something active, do stop.py
    (or just do stop.py)

--- also ---

How to deal with multiple preople using a single observatory?
Users could issue start/stop commands from different head nodes
(e.g. multiple nodes configured to be a head node)

for starters:
start/stop commands: make sure they're being run on the head node
specified in obs_config.json

If someone is doing a run,
other start.py commands will fail because of the above.
Tell the user: it's possible that someone else is
using the observatory.
If this is possible, please check for this before doing stop.py

Note: it a user runs stop.py, it stop recording any other user's run

--- also ---

we do start/stop stuff remotely by using ssh to run *_daq.py.
But what if head node = DAQ node, i.e. one node?
We could avoid ssh'ing to the same node;
Instead package code in *_daq.py and call it from script

Maybe better to always use SSH - uniformity.

So various possibilities:

1) single node - both head and DAQ
2) multiple nodes; head node is also DAQ node
3) multiple nodes: head node is not a DAQ node

daq_config.json specifies:
    - IP and data dir for head node
    - IP, username and data dir for each DAQ node

start.py: check that we're running on head node

--- also ---

On head node:
    command dir: where you run start/stop/status commands from.
        typically source dir, but could be elsewhere.
        config files must be here
        No files are created here.
    data dir
        All files are created here
        run dirs
        current run name

on DAQ nodes
    data dir
        run dirs
        if remote:
            *_daq.py source code is copied here
            keep track of current run in "current_run_daq".
----------
What are quabo versions at UCB?
    mixed
    pixel order is only difference
    
Add command to show silver version (after reboot)

reboot didn't get HK packet

config reboot: try HK port bind at start
    ping first

-----------
errors
    missing symlinks etc.
    reboot from wrong IP addr
stop run if hashpipe fails
daemons
    no error if running
redundant stuff in HK
----------------

send PH threshold to boards
GAIN
DAC1
    convert pe_threshold to DAC1

send calibration

HV settings

function to mask pixels
chan mask: prevent PH triggering per pixel

data viewer for PFF files

to generate quabo config (matlab):
    (dark telescope)
    vary DAC1 from 150 to 250 by 1
        look for steps in downward slope
        PH, image:
    generates: 
        .mat file
        .txt file

remove reboot check

config file:
    IP addr of WR
    UART port of GPS receiver

same data dir on head/daq node

"this is not the head node spec..."
---------------
4/7
if only PH or only image
    DAC1
if both
    DAC1 image
    DAC2 ph

4/21
Matt security
OS is current; auto updates or once/week
    current Apache, PHP, Python
apt install sshguard: blocks repeated SSH failures
accounts:
    separate; if shared acct, don't allow direct login
john the ripper: password checker

use proxy server so firewall only has 1 client

.htaccess

make sure head node IP works
--------------
Interface for flash in config (done)

web interface:
    histogram of values per file
    make video w/ min/max limits

copy/process: keep track of what's done
    assume if dir exists locally, it's done

5/26/22
todo:
start.py creates symlink 'run' to run dur
stop.py creates symlinks 'img' and 'ph' to first files in run dir
show_pff.py
    auto-calibrate to 5-95% range
    default to 'img'

6/27/22
    add nanosec timestamp to video
    make framebrowser for PH mode

8/25/22
hashpipe.py: make BINDHOST a param in daq_config.py
start.py without HV on?
    don't run Nico script either
start.py
    send data_packet_dest cmd

9/19/22
    coincidence between image pulse
    repeated events within 1 obs run (different pixels)
        divide into pixel
        do it on the fly?

    clouds?
        autodetect?

    keep log of sky coverage?
------------
improved record-keeping of run end

Ending a run has several steps
- tell DAQ nodes to stop recording data
- turning off HV updater and HK recorder
- tell quabos to stop producing data
- copy files from DAQ nodes to head node
- delete files from DAQ nodes

Any of these can fail for reasons that may be transient.
The collect step can take several hours.

Currently we write a file "run_complete" at the end of all this;
its contents are the time when everything finished.

Problems:
1) There's no easy way to find when data recording stopped.
2) If something failed, there's no info about what it was,
    and no easy way to retry.

Proposal:
files to write at end of a run:

recording_ended
    time when stopped recording data

collect_complete
    time when collect completed

run_complete
    time when everything completed

stop_errors
    log of errors, if any

Change stop.py so that it doesn't repeat completed steps.
-------------
RA format
fixed-size JSON headers
    function to seek
------
Data with birdies: proposal

Currently our dir structure is:

data/
    obs_run/
        files
analysis/
    obs_run/
        analysis_type/
            analysis_1
            analysis_2

Where does birdie data go?
I propose that it go under data/, same as real data.
There are two cases:
1) birdie data derived from real data
    Say, the first 5 minutes of an image-mode file with some added flashes.
    Put this in a dir with the same name as the real data,
    with 'birdie_N' added.
    For example
    obs_Lick.start_2022-10-14T21:45:29Z.runtype_eng.birdie_0.pffd
    N is a sequence number in case you use the same data multiple times
2) synthetic data
    Put it in a dir with obs = 'synthetic': e.g.
    obs_synthetic.start_2022-10-14T21:45:29Z.runtype_eng.pffd

In both cases the directory should contain a JSON file "birdies.json"
describing the birdies that were added
(and in the synthetic case, the parameters of the background noise)

This scheme lets use run analyses on birdie data,
with no modifications to the analysis framework.
The results of analysis of birdie data will go under
analysis/
    obs_run
where obs_run is the name of the birdie data dir
----------------
Image headers have (for each quabo)
    pkt_tai
    pkt_nsec
        TAI is from white rabbit; it's unix time + 37
        pkt_tai is the low 10 bits of seconds
    tv_sec
    tv_usec
        unix time on the DAQ node
These should agree within a few milliseconds; if not something is wrong

We have only the low 10 bits of WR seconds
How to go from these to (full) WR time?
The following don't work:
- tv_sec + pkt_nsec
    this will be off by 1 sec for a few milliseconds every second
    (when WR has wrapped around but Unix time hasn't, or vice versa)
- tv_sec with low 10 bits replaced by (pkt_tai-37)%1024 + pkt_nsec
    this will be off by 1024 sec for a few milliseconds every 1024 seconds

Instead:

d = (tv_usec - tai_sec + 37)%1024

if d == 0:
    x = tv_usec + tai_nsec
elif d == 1:
    x = tv_usec-1 + tai_nsec
elif d == 1023:
    x = tv_usec+1 + tai_nsec
else:
    error: WR/Unix clocks are off by more than 1 sec
